{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Forward Propagation for SEN12MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.sen12ms import SEN12MS, Normalize, ToTensor, bands_mean, bands_std, CLASS_NAMES;\n",
    "from models.networks.resnet import ResNet50;\n",
    "from models.network_mapper import to_relevance_representation, to_basic_representation;\n",
    "from utils.Utils import input_mapping, set_seed\n",
    "import matplotlib.pyplot as plt;\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from torch.utils.data import DataLoader;\n",
    "import seaborn as sns\n",
    "import torchvision.transforms as transforms;\n",
    "import os\n",
    "from config.config import label_split_dir_cfg, data_dir_sen12mscr_cfg, data_dir_sen12ms_cfg, ckpt_path_resnet50_sen12ms_cfg\n",
    "\n",
    "from captum.attr import LRP\n",
    "\n",
    "\n",
    "import torch;\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Model\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| ckpt_path | string | Path to model checkpoint|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = ckpt_path_resnet50_sen12ms_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model = ResNet50(n_inputs=12, num_classes=10)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "#### Load Test Data\n",
    "**Data Setup**\n",
    "| Variable | Data Type | Explanation |\n",
    "| --- | --- | --- |\n",
    "| data_dir_clear | string | root path of SEN12MS |\n",
    "| data_dir_cloudy | string | root path of SEN12MSCR |\n",
    "| label_split_dir | string | path to label_split pickle file |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "data_dir_clear = data_dir_sen12ms_cfg\n",
    "data_dir_cloudy = data_dir_sen12mscr_cfg\n",
    "label_split_dir = label_split_dir_cfg\n",
    "\n",
    "batch_size = 16\n",
    "img_transform = transforms.Compose([ToTensor(),\n",
    "                                     Normalize(bands_mean, \n",
    "                                               bands_std)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Clear and Cloudy Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "test_data = SEN12MS(path_clear=data_dir_clear, \n",
    "                    path_cloudy=data_dir_cloudy,\n",
    "                    ls_dir=label_split_dir,\n",
    "                    img_transform=img_transform,\n",
    "                    label_type=\"single_label\",\n",
    "                    subset=\"test\",\n",
    "                    use_cloudy=True,\n",
    "                    cloud_frac=1,\n",
    "                    use_s1=True,\n",
    "                    use_s2=True)\n",
    "  \n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Visualization\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| num_examples | [num_rows, num_cols] | Number of rows and columns of example images |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "num_examples = [5,5]\n",
    "figsize = [15,15]\n",
    "fontsize = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Examples Clear and Cloudy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "test_data.use_cloudy = True\n",
    "fig, axs = plt.subplots(*num_examples, figsize=figsize)\n",
    "test_data.plot_imgs(axs, num_examples, fontsize=fontsize)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Evaluate Clear Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate Model with Clear Test Set\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = model.double().to(device)\n",
    "model.eval()\n",
    "\n",
    "test_data.use_cloudy = False\n",
    "test_data.cloud_frac = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_correct_count_clear = 0\n",
    "  test_total_count_clear = 1e-20\n",
    "  test_loss_sum_clear = 0\n",
    "  test_pred_list_clear = []\n",
    "  test_labels_list_clear = []\n",
    "  test_id_list_clear = []\n",
    "   \n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    Acc {test_correct_count_clear/test_total_count_clear}\", end=\"\\r\")\n",
    "    label = data[\"label\"].argmax(-1).to(device)\n",
    "    x = data[\"image\"].double().to(device)\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    test_pred_list_clear.append(pred.cpu())\n",
    "    test_loss = criterion(pred, label)\n",
    "\n",
    "    test_id_list_clear += data[\"id\"]\n",
    "    test_labels_list_clear.append(label.cpu())\n",
    "    test_loss_sum_clear += test_loss.cpu().item()\n",
    "    test_correct_count_clear += (label == pred.argmax(-1)).sum().cpu().item()\n",
    "    test_total_count_clear += x.shape[-4]\n",
    "\n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {test_loss_sum_clear/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {test_correct_count_clear/test_total_count_clear:.4f}\")\n",
    "  \n",
    "  test_pred_list_clear = torch.cat(test_pred_list_clear, 0)\n",
    "  test_labels_list_clear = torch.cat(test_labels_list_clear, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save / Load Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to save\n",
    "os.makedirs(\"./tmp\", exist_ok=True)\n",
    "torch.save({\"test_loss_sum_clear\": test_loss_sum_clear,\n",
    "            \"num_batches\": len(test_data_loader),\n",
    "            \"test_correct_count_clear\": test_correct_count_clear,\n",
    "            \"test_total_count_clear\": test_total_count_clear,\n",
    "            \"test_pred_list_clear\": test_pred_list_clear,\n",
    "            \"test_labels_list_clear\": test_labels_list_clear,\n",
    "            \"test_id_list_clear\": test_id_list_clear}, \"./tmp/original_normal_clear.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = torch.load(\"./tmp/original_normal_clear.pth\")\n",
    "test_loss_sum_clear = res_dict[\"test_loss_sum_clear\"]\n",
    "test_num_batches_clear = res_dict[\"num_batches\"]\n",
    "test_correct_count_clear = res_dict[\"test_correct_count_clear\"]\n",
    "test_total_count_clear = res_dict[\"test_total_count_clear\"]\n",
    "test_pred_list_clear = res_dict[\"test_pred_list_clear\"]\n",
    "test_labels_list_clear = res_dict[\"test_labels_list_clear\"]\n",
    "test_id_list_clear = res_dict[\"test_id_list_clear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Cloudy Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate Model with Cloudy Test Set\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = model.double().to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_data.use_cloudy = True\n",
    "test_data.cloud_frac = 1.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  test_correct_count_cloudy = 0\n",
    "  test_total_count_cloudy = 1e-20\n",
    "  test_loss_sum_cloudy = 0\n",
    "  test_pred_list_cloudy = []\n",
    "  test_labels_list_cloudy = []\n",
    "  test_id_list_cloudy = []\n",
    "  \n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    Acc {test_correct_count_cloudy/test_total_count_cloudy}\", end=\"\\r\")\n",
    "    label = data[\"label\"].argmax(-1).to(device)\n",
    "    x = data[\"image\"].double().to(device)\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    test_pred_list_cloudy.append(pred.cpu())\n",
    "    test_loss = criterion(pred, label)\n",
    "\n",
    "    test_id_list_cloudy += data[\"id\"]\n",
    "    test_labels_list_cloudy.append(label.cpu())\n",
    "    test_loss_sum_cloudy += test_loss.cpu().item()\n",
    "    test_correct_count_cloudy += (label == pred.argmax(-1)).sum().cpu().item()\n",
    "    test_total_count_cloudy += x.shape[-4]\n",
    "\n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {test_loss_sum_cloudy/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {test_correct_count_cloudy/test_total_count_cloudy:.4f}\")\n",
    "  \n",
    "  test_pred_list_cloudy = torch.cat(test_pred_list_cloudy, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save / Load Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to save\n",
    "os.makedirs(\"./tmp\", exist_ok=True)\n",
    "torch.save({\"test_loss_sum_cloudy\": test_loss_sum_cloudy,\n",
    "            \"num_batches\": len(test_data_loader),\n",
    "            \"test_correct_count_cloudy\": test_correct_count_cloudy,\n",
    "            \"test_total_count_cloudy\": test_total_count_cloudy,\n",
    "            \"test_pred_list_cloudy\": test_pred_list_cloudy,\n",
    "            \"test_labels_list_cloudy\": test_labels_list_cloudy,\n",
    "            \"test_id_list_cloudy\": test_id_list_cloudy}, \"./tmp/original_normal_cloudy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to load\n",
    "res_dict = torch.load(\"./tmp/original_normal_cloudy.pth\")\n",
    "test_loss_sum_cloudy = res_dict[\"test_loss_sum_cloudy\"]\n",
    "test_num_batches_cloudy = res_dict[\"num_batches\"]\n",
    "test_correct_count_cloudy = res_dict[\"test_correct_count_cloudy\"]\n",
    "test_total_count_cloudy = res_dict[\"test_total_count_cloudy\"]\n",
    "test_pred_list_cloudy = res_dict[\"test_pred_list_cloudy\"]\n",
    "test_labels_list_cloudy = res_dict[\"test_labels_list_cloudy\"]\n",
    "test_id_list_cloudy = res_dict[\"test_id_list_cloudy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Clear Data Set on RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate RFP Model with Clear Test Set\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = to_relevance_representation(model, verbose=0).double().to(device)\n",
    "model.eval()\n",
    "\n",
    "test_data.use_cloudy = False\n",
    "test_data.cloud_frac = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  rfp_test_correct_count_clear = 0\n",
    "  rfp_test_total_count_clear = 1e-20\n",
    "  rfp_test_loss_sum_clear = 0\n",
    "  rfp_test_pred_list_clear = []\n",
    "  rfp_test_labels_list_clear = []\n",
    "  rfp_test_id_list_clear = []\n",
    "\n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    Acc {rfp_test_correct_count_clear/rfp_test_total_count_clear}\", end=\"\\r\")\n",
    "    label = data[\"label\"].argmax(-1).to(device)\n",
    "\n",
    "    x1, x2 = data[\"image\"].split([10,2], dim=-3)\n",
    "    x1, x2 = input_mapping(x1, x2)\n",
    "    x = torch.cat([x1,x2], -3).double().to(device=device)\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    rfp_test_pred_list_clear.append(pred.cpu())\n",
    "    test_loss = criterion(pred.sum(0), label)\n",
    "    rfp_test_labels_list_clear.append(label.cpu())\n",
    "    rfp_test_id_list_clear += data[\"id\"]\n",
    "    \n",
    "    rfp_test_loss_sum_clear += test_loss.cpu().item()\n",
    "    rfp_test_correct_count_clear += (label == pred.sum(0).argmax(-1)).sum().cpu().item()\n",
    "    rfp_test_total_count_clear += x.shape[-4]\n",
    "\n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {rfp_test_loss_sum_clear/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {rfp_test_correct_count_clear/rfp_test_total_count_clear:.4f}\")\n",
    "  \n",
    "  rfp_test_pred_list_clear = torch.cat(rfp_test_pred_list_clear, 1)\n",
    "  rfp_test_labels_list_clear = torch.cat(rfp_test_labels_list_clear, 0)\n",
    "model = to_basic_representation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to save\n",
    "os.makedirs(\"./tmp\", exist_ok=True)\n",
    "torch.save({\"rfp_test_loss_sum_clear\": rfp_test_loss_sum_clear,\n",
    "            \"num_batches\": len(test_data_loader),\n",
    "            \"rfp_test_correct_count_clear\": rfp_test_correct_count_clear,\n",
    "            \"rfp_test_total_count_clear\": rfp_test_total_count_clear,\n",
    "            \"rfp_test_pred_list_clear\": rfp_test_pred_list_clear,\n",
    "            \"rfp_test_labels_list_clear\": rfp_test_labels_list_clear,\n",
    "            \"rfp_test_id_list_clear\": rfp_test_id_list_clear}, \"./tmp/rfp_clear.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to load\n",
    "res_dict = torch.load(\"./tmp_double/rfp_clear.pth\")\n",
    "print(res_dict.keys())\n",
    "rfp_test_loss_sum_clear = res_dict[\"rfp_test_loss_sum_clear\"]\n",
    "rfp_test_num_batches_clear = res_dict[\"num_batches\"]\n",
    "rfp_test_correct_count_clear = res_dict[\"rfp_test_correct_count_clear\"]\n",
    "rfp_test_total_count_clear = res_dict[\"rfp_test_total_count_clear\"]\n",
    "rfp_test_pred_list_clear = res_dict[\"rfp_test_pred_list_clear\"]\n",
    "rfp_test_labels_list_clear = res_dict[\"rfp_test_labels_list_clear\"]\n",
    "rfp_test_id_list_clear = res_dict[\"rfp_test_id_list_clear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Cloudy Data Set on RFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate RFP Model with Cloudy Test Set\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = to_relevance_representation(model, verbose=0).double().to(device)\n",
    "model.eval()\n",
    "\n",
    "test_data.use_cloudy = True\n",
    "test_data.cloud_frac = 1\n",
    "\n",
    "with torch.no_grad():\n",
    "  rfp_test_correct_count_cloudy = 0\n",
    "  rfp_test_total_count_cloudy = 1e-20\n",
    "  rfp_test_loss_sum_cloudy = 0\n",
    "  rfp_test_pred_list_cloudy = []\n",
    "  rfp_test_labels_list_cloudy = []\n",
    "  rfp_test_id_list_cloudy = []\n",
    "  \n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    Acc {rfp_test_correct_count_cloudy/rfp_test_total_count_cloudy}\", end=\"\\r\")\n",
    "    label = data[\"label\"].argmax(-1).to(device)\n",
    "    x = data[\"image\"].double().to(device)\n",
    "    x1, x2 = data[\"image\"].double().split([10,2], dim=-3)\n",
    "    x1, x2 = input_mapping(x1, x2)\n",
    "    x = torch.cat([x1,x2], -3).to(device=device)\n",
    "\n",
    "    pred = model(x)\n",
    "\n",
    "    rfp_test_pred_list_cloudy.append(pred.cpu())\n",
    "    test_loss = criterion(pred.sum(0), label)\n",
    "    rfp_test_labels_list_cloudy.append(label.cpu())\n",
    "    rfp_test_id_list_cloudy += data[\"id\"]\n",
    "\n",
    "    rfp_test_loss_sum_cloudy += test_loss.cpu().item()\n",
    "    rfp_test_correct_count_cloudy += (label == pred.sum(0).argmax(-1)).sum().cpu().item()\n",
    "    rfp_test_total_count_cloudy += x.shape[-4]\n",
    "\n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {rfp_test_loss_sum_cloudy/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {rfp_test_correct_count_cloudy/rfp_test_total_count_cloudy:.4f}\")\n",
    "  \n",
    "  rfp_test_pred_list_cloudy = torch.cat(rfp_test_pred_list_cloudy, 1)\n",
    "  rfp_test_labels_list_cloudy = torch.cat(rfp_test_labels_list_cloudy, 0)\n",
    "\n",
    "model = to_basic_representation(model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save / Load Intermediate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to save\n",
    "os.makedirs(\"./tmp\", exist_ok=True)\n",
    "torch.save({\"rfp_test_loss_sum_cloudy\": rfp_test_loss_sum_cloudy,\n",
    "            \"num_batches\": len(test_data_loader),\n",
    "            \"rfp_test_correct_count_cloudy\": rfp_test_correct_count_cloudy,\n",
    "            \"rfp_test_total_count_cloudy\": rfp_test_total_count_cloudy,\n",
    "            \"rfp_test_pred_list_cloudy\": rfp_test_pred_list_cloudy,\n",
    "            \"rfp_test_labels_list_cloudy\": rfp_test_labels_list_cloudy,\n",
    "            \"rfp_test_id_list_cloudy\": rfp_test_id_list_cloudy}, \"./tmp/rfp_cloudy.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cell to load\n",
    "res_dict = torch.load(\"./tmp_double/rfp_cloudy.pth\")\n",
    "rfp_test_loss_sum_cloudy = res_dict[\"rfp_test_loss_sum_cloudy\"]\n",
    "rfp_test_num_batches_cloudy = res_dict[\"num_batches\"]\n",
    "rfp_test_correct_count_cloudy = res_dict[\"rfp_test_correct_count_cloudy\"]\n",
    "rfp_test_total_count_cloudy = res_dict[\"rfp_test_total_count_cloudy\"]\n",
    "rfp_test_pred_list_cloudy = res_dict[\"rfp_test_pred_list_cloudy\"]\n",
    "rfp_test_labels_list_cloudy = res_dict[\"rfp_test_labels_list_cloudy\"]\n",
    "rfp_test_id_list_cloudy = res_dict[\"rfp_test_id_list_cloudy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Evaluation between RFP and Basic Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity Evaluation RFP and Basic Model\")\n",
    "print()\n",
    "print(\"Output shapes\")\n",
    "print(f\"    Basic Model:   Test Clear {test_pred_list_clear.shape}   Test Cloudy {test_pred_list_cloudy.shape}\")\n",
    "print(f\"      RFP Model:   Test Clear {rfp_test_pred_list_clear.shape}   Test Cloudy {rfp_test_pred_list_cloudy.shape}\")\n",
    "print(f\"Loss:\")\n",
    "print(f\"    Basic Model:   Test Clear {test_loss_sum_clear/test_num_batches_clear:.6f}   Test Cloudy {test_loss_sum_cloudy/test_num_batches_cloudy:.6f}\")\n",
    "print(f\"      RFP Model:   Test Clear {rfp_test_loss_sum_clear/rfp_test_num_batches_clear:.6f}   Test Cloudy {rfp_test_loss_sum_cloudy/rfp_test_num_batches_cloudy:.6f}\")\n",
    "print(f\"          Delta:   Test Clear {(rfp_test_loss_sum_clear/rfp_test_num_batches_clear-test_loss_sum_clear/test_num_batches_clear):.6f}   Test Cloudy {(test_loss_sum_cloudy/test_num_batches_cloudy-rfp_test_loss_sum_cloudy/rfp_test_num_batches_cloudy):.6f}\")\n",
    "print(\"Model Output\")\n",
    "print(f\"    Mean L1-error:   Test Clear {(test_pred_list_clear-rfp_test_pred_list_clear.sum(0)).abs().mean()}   Test Cloudy {(test_pred_list_cloudy-rfp_test_pred_list_cloudy.sum(0)).abs().mean()}\")\n",
    "print(f\"     Std L1-error:   Test Clear {(test_pred_list_clear-rfp_test_pred_list_clear.sum(0)).abs().std()}   Test Cloudy {(test_pred_list_cloudy-rfp_test_pred_list_cloudy.sum(0)).abs().std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Visualizations Clear\n",
    "Visualization of sample-wise relevance of data source 1 (L), data source 2 (R) and the sample-wise difference (L-R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "labels_str = [\"Bias\", \"SAR\", \"Optical\"]\n",
    "ncols = 2\n",
    "nrows = 100\n",
    "\n",
    "plot_idx = torch.randperm(len(test_data))\n",
    "plot_idx = [idx for idx in plot_idx if \"s2_cloudy\" in test_data.samples[idx] \n",
    "                                        and test_data.samples[idx][\"s2_cloudy\"] is not None \n",
    "                                        and test_data.labels_filtered[test_data.samples[idx][\"id\"]] != 2]\n",
    "plot_idx = plot_idx[:ncols*nrows]\n",
    "\n",
    "for sub_sample_id in plot_idx:\n",
    "\n",
    "    s = test_data.samples[sub_sample_id][\"id\"]\n",
    "\n",
    "    sample = test_data.get_print_data(sub_sample_id)\n",
    "    sample_name = sample[\"id\"]\n",
    "    label = sample[\"label\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(14,10))\n",
    "    gs = GridSpec(2, 2, figure=fig, height_ratios=[0.5,0.5], width_ratios=[0.5,0.5])\n",
    "\n",
    "    ax11 = fig.add_subplot(gs[0:2, 0])\n",
    "    ax13 = fig.add_subplot(gs[0, 1])        \n",
    "    ax23 = fig.add_subplot(gs[1, 1])       \n",
    "\n",
    "    img_opt_clear = sample[\"img1_clear\"]\n",
    "    img_opt_cloudy = sample[\"img1_cloudy\"]\n",
    "    img_sar = sample[\"img2\"]\n",
    "\n",
    "    img = torch.cat([\n",
    "        torch.cat([img_opt_clear, torch.ones([10, img_sar.shape[0],3]), img_opt_cloudy],0),\n",
    "        torch.ones([img_sar.shape[0]*2 + 10, 10, 3]),\n",
    "        torch.cat([torch.ones_like(img_sar), torch.ones([10, img_sar.shape[0],3]), img_sar],0)], 1)\n",
    "\n",
    "    ax11.axis('off')\n",
    "    ax11.matshow(img)\n",
    "    \n",
    "    pred_pos_clear = rfp_test_id_list_clear.index(sample_name)\n",
    "    pred_pos_cloudy = rfp_test_id_list_cloudy.index(sample_name)\n",
    "\n",
    "    text_size = 18\n",
    "    ax13.bar(range(10), rfp_test_pred_list_clear[0,pred_pos_clear], label=labels_str[0], color='r')\n",
    "    ax13.bar(range(10), rfp_test_pred_list_clear[1,pred_pos_clear].clip(min=0), width=0.8, bottom=rfp_test_pred_list_clear[:1,pred_pos_clear].clip(min=0).sum(0), label=labels_str[2], color='g')\n",
    "    ax13.bar(range(10), rfp_test_pred_list_clear[1,pred_pos_clear].clip(max=0), width=0.8, bottom=rfp_test_pred_list_clear[:1,pred_pos_clear].clip(max=0).sum(0), color='g')\n",
    "    ax13.bar(range(10), rfp_test_pred_list_clear[2,pred_pos_clear].clip(min=0), width=0.8, bottom=rfp_test_pred_list_clear[:2,pred_pos_clear].clip(min=0).sum(0), label=labels_str[1], color='b')\n",
    "    ax13.bar(range(10), rfp_test_pred_list_clear[2,pred_pos_clear].clip(max=0), width=0.8, bottom=rfp_test_pred_list_clear[:2,pred_pos_clear].clip(max=0).sum(0), color='b')\n",
    "    ax13.bar(torch.arange(10)+0.2, rfp_test_pred_list_clear[:3,pred_pos_clear].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax13.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "\n",
    "    ax13.set_ylim([-40,40]) \n",
    "    ax13.tick_params(axis='x', labelsize=text_size)\n",
    "    ax13.tick_params(axis='y', labelsize=text_size)\n",
    "    ax13.set_xticks(range(10), CLASS_NAMES, rotation=45, ha=\"right\", fontsize=text_size)\n",
    "\n",
    "    text_size = 18\n",
    "    ax23.bar(range(10), rfp_test_pred_list_cloudy[0,pred_pos_cloudy], label=labels_str[0], color='r')\n",
    "    ax23.bar(range(10), rfp_test_pred_list_cloudy[1,pred_pos_cloudy].clip(min=0), width=0.8, bottom=rfp_test_pred_list_cloudy[:1,pred_pos_cloudy].clip(min=0).sum(0), label=labels_str[2], color='g')\n",
    "    ax23.bar(range(10), rfp_test_pred_list_cloudy[1,pred_pos_cloudy].clip(max=0), width=0.8, bottom=rfp_test_pred_list_cloudy[:1,pred_pos_cloudy].clip(max=0).sum(0), color='g')\n",
    "    ax23.bar(range(10), rfp_test_pred_list_cloudy[2,pred_pos_cloudy].clip(min=0), width=0.8, bottom=rfp_test_pred_list_cloudy[:2,pred_pos_cloudy].clip(min=0).sum(0), label=labels_str[1], color='b')\n",
    "    ax23.bar(range(10), rfp_test_pred_list_cloudy[2,pred_pos_cloudy].clip(max=0), width=0.8, bottom=rfp_test_pred_list_cloudy[:2,pred_pos_cloudy].clip(max=0).sum(0), color='b')\n",
    "    ax23.bar(torch.arange(10)+0.2, rfp_test_pred_list_cloudy[:3,pred_pos_cloudy].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax23.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "    ax23.tick_params(axis='x', labelsize=text_size)\n",
    "    ax23.tick_params(axis='y', labelsize=text_size)\n",
    "    ax23.set_ylim([-40,40]) \n",
    "    ax23.set_xticks(range(10), CLASS_NAMES, rotation=45, ha=\"right\", fontsize=text_size)\n",
    "\n",
    "    ax11.axis('off')\n",
    "    ax11.legend(*ax13.get_legend_handles_labels(), fontsize=text_size, loc=\"upper right\")\n",
    "    ax11.set_title(f\"\\nTrue Class: {CLASS_NAMES[label]}\" + \\\n",
    "                f\"\\nClear Prediction: {CLASS_NAMES[rfp_test_pred_list_clear.sum(0)[pred_pos_clear].argmax(-1).item()]}\" + \\\n",
    "                f\"\\nCloudy Prediction:  {CLASS_NAMES[rfp_test_pred_list_cloudy.sum(0)[pred_pos_cloudy].argmax(-1).item()]}\",\n",
    "                fontsize=text_size)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(\"SEN12MS_examples\", exist_ok=True)\n",
    "    plt.savefig(os.path.join(\"SEN12MS_examples\", f\"SEN12MS_examples_{pred_pos_clear}.pdf\"))    \n",
    "    plt.savefig(os.path.join(\"SEN12MS_examples\", f\"SEN12MS_examples_{pred_pos_clear}.png\"), dpi=300)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
