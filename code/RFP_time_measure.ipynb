{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/code\")\n",
    "sys.path.append(\"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/\")\n",
    "import time \n",
    "import torch\n",
    "from captum.attr import LRP\n",
    "\n",
    "from models.NetworkMapper import to_base_propagation, to_contribution_propagation\n",
    "from datasets.MNIST import MNIST, MNIST_SUM, MNIST_DataNoise, MNIST_LabelNoise, MNIST_ImgShuffle;\n",
    "from ContributionPropagation.Contribution import PerceptualScore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_shape=[28,28], nclasses=10):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.nclasses = nclasses\n",
    "        self.branch1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            #torch.nn.Dropout(0.25),\n",
    "            )   #torch.nn.Flatten(-3))\n",
    "\n",
    "        self.branch2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            #torch.nn.Dropout(0.25),\n",
    "            ) #torch.nn.Flatten(-3)) # 12 x 12 x 64\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(2 * ((input_shape[0]-4) // 2) * ((input_shape[1]-4) // 2) * 64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, nclasses)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        #assert type(input) is list\n",
    "        if len(input) != 2 and len(input[0]) == 2: \n",
    "            input = input.permute([1,0,2,3,4])\n",
    "        assert len(input) == 2\n",
    "        \n",
    "        x1_in, x2_in = input[0], input[1]\n",
    "\n",
    "        x1 = self.branch1(x1_in)\n",
    "        x2 = self.branch2(x2_in)\n",
    "\n",
    "        x1 = torch.flatten(x1, start_dim=1)\n",
    "        x2 = torch.flatten(x2, start_dim=1)\n",
    "\n",
    "        x = torch.cat([x1,x2], -1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 3 facor 1 of 3\n",
      "Applied monkey patches for contribution propagation.\n",
      " Call Recursive:  ('branch1', Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "))\n",
      " Call Recursive:  ('branch2', Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed all monkey patches for contribution propagation.\n",
      " Call Recursive:  ('branch1', Sequential(\n",
      "  (0): Conv2d(\n",
      "    (layer): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(\n",
      "    (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(\n",
      "    (unfold): Unfold(kernel_size=(2, 2), dilation=1, padding=0, stride=[2, 2])\n",
      "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "))\n",
      " Call Recursive:  ('branch2', Sequential(\n",
      "  (0): Conv2d(\n",
      "    (layer): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(\n",
      "    (layer): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(\n",
      "    (unfold): Unfold(kernel_size=(2, 2), dilation=1, padding=0, stride=[2, 2])\n",
      "    (layer): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "))\n",
      "Seed: 3 facor 1 of 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied monkey patches for contribution propagation.\n",
      " Call Recursive:  ('branch1', Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=(2, 2), stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "))\n",
      " Call Recursive:  ('branch2', Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=(2, 2), stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
      "))\n",
      "Seed: 3 facor 1 of 3\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for no_gpu in [True]:\n",
    "  if no_gpu: device = \"cpu\"\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for all_at_once in [True]:\n",
    "      for inv1 in [False]:\n",
    "        for inv2 in [False]:\n",
    "          for split in [True]:\n",
    "            for seed in [3]:\n",
    "              # for factor in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32]:\n",
    "              f_list = [150, 250, 500] #[1,2,5,10,15,25,50,100,150,250,500]\n",
    "              for f_id, factor in enumerate(f_list): #[40, 50, 75, 100, 200, 250, 500, 1000]:\n",
    "\n",
    "                print(f\"Seed: {seed} facor {f_id+1} of {len(f_list)}\")\n",
    "                input_shape = [28,14] if split else [28,28]\n",
    "              \n",
    "                model = Net(input_shape=input_shape)\n",
    "\n",
    "                exp_folder = \"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/0001_MNIST_Train_and_Evaluate/Experiments/Basic\"\n",
    "                exp_path = os.path.join(exp_folder, f\"basic_img_experiments_seed{seed}_inv1{inv1}_inv2{inv2}_split{split}.pth\")\n",
    "\n",
    "                res_file = torch.load(exp_path, map_location=\"cpu\")\n",
    "\n",
    "                model_state_dict = res_file[\"final_model_state_dict\"]\n",
    "\n",
    "                model.load_state_dict(model_state_dict)\n",
    "\n",
    "                model = to_contribution_propagation(model).to(device)\n",
    "\n",
    "                data = MNIST(train=False, split_image=split, invert1=inv1, invert2=inv2);\n",
    "\n",
    "                img1_tensor = torch.stack([data[i][\"img1\"] for i in range(len(data))],0)\n",
    "                img2_tensor = torch.stack([data[i][\"img2\"] for i in range(len(data))],0)\n",
    "                labels = torch.as_tensor([data[i][\"label\"] for i in range(len(data))]).unsqueeze(1)\n",
    "\n",
    "                num_samples_list = [1, 5, 10, 25, 50, 100, 250] #, 500]\n",
    "\n",
    "                times = {\"prediction_single\": [],\n",
    "                        \"RFP\": [],\n",
    "                        \"shape\": [],\n",
    "                        \"perc\": dict([(i, []) for i in num_samples_list]),\n",
    "                        \"lrp\": [],\n",
    "                        \"lrp_single\": []\n",
    "                        }\n",
    "\n",
    "                model = to_base_propagation(model)\n",
    "\n",
    "                if not all_at_once:\n",
    "                  for s in range(len(data)):\n",
    "                    print(f\"Seed {seed}: {s+1} of {len(data)}                 \", end=\"\\r\")\n",
    "\n",
    "                    # ImageClassifier takes a single input tensor of images Nx3x32x32,\n",
    "                    # and returns an Nx10 tensor of class probabilities. It has one\n",
    "                    # Conv2D and a ReLU layer.\n",
    "\n",
    "                    lrp = LRP(model)\n",
    "                    # Attribution size matches input size: 3x3x32x32\n",
    "\n",
    "                    img1 = img1_tensor[s].unsqueeze(0)\n",
    "                    img2 = img2_tensor[s].unsqueeze(0)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    attribution = lrp.attribute(torch.stack([img1, img2],0).to(device), target=5)\n",
    "                    times[\"lrp_single\"].append(time.time()-start_time)\n",
    "                    del attribution\n",
    "\n",
    "                    img1 = img1_tensor[s].unsqueeze(0)\n",
    "                    img2 = img2_tensor[s].unsqueeze(0)\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    attribution = lrp.attribute(torch.repeat_interleave(torch.stack([img1, img2],0), 10, dim=1).permute([1,0,2,3,4]).to(device), target=[0,1,2,3,4,5,6,7,8,9])\n",
    "                    times[\"lrp\"].append(time.time()-start_time)\n",
    "                    del attribution\n",
    "\n",
    "                  model = to_contribution_propagation(model).to(device)\n",
    "\n",
    "                  for s in range(len(data)):\n",
    "                    print(f\"Seed {seed}: {s+1} of {len(data)}                 \", end=\"\\r\")\n",
    "                    \n",
    "                    # SHAPE Score\n",
    "\n",
    "                    img1 = img1_tensor[s].unsqueeze(0)\n",
    "                    img2 = img2_tensor[s].unsqueeze(0)  \n",
    "\n",
    "                    zeros1 = torch.zeros_like(img1)\n",
    "                    zeros2 = torch.zeros_like(img2)\n",
    "\n",
    "                    # First Input:    Original combination\n",
    "                    # Second part:    modality 2 random\n",
    "                    #  Third part:    modality 1 random\n",
    "                    x1 = [img1, img1, zeros1]\n",
    "                    x2 = [img2, zeros2, img2]\n",
    "\n",
    "                    x1 = torch.cat(x1, 0)\n",
    "                    x2 = torch.cat(x2, 0)\n",
    "\n",
    "                    start_time = time.time() \n",
    "                    _ =  model([x1.to(device),x2.to(device)])\n",
    "                    times[\"shape\"].append(time.time() - start_time) \n",
    "\n",
    "\n",
    "                    # Single Prediction\n",
    "                    img1 = img1_tensor[s].unsqueeze(0)\n",
    "                    img2 = img2_tensor[s].unsqueeze(0)  \n",
    "                    start_time = time.time()          \n",
    "                    _ =  model([img1.to(device), img2.to(device)])\n",
    "                    times[\"prediction_single\"].append(time.time() - start_time)\n",
    "                    del img1\n",
    "                    del img2\n",
    "\n",
    "                    img1 = img1_tensor[s].unsqueeze(0)\n",
    "                    img2 = img2_tensor[s].unsqueeze(0)  \n",
    "                    x1 = torch.stack([\n",
    "                                        torch.zeros_like(img1),\n",
    "                                        img1,\n",
    "                                        torch.zeros_like(img1)\n",
    "                                    ], 0)\n",
    "                    \n",
    "                    x2 = torch.stack([\n",
    "                                        torch.zeros_like(img2),\n",
    "                                        torch.zeros_like(img2),\n",
    "                                        img2\n",
    "                                    ], 0)       \n",
    "                        \n",
    "                    # Relevance Forward Propagation\n",
    "                    start_time = time.time() \n",
    "                    _ =  model([x1.to(device),x2.to(device)])\n",
    "                    times[\"RFP\"].append(time.time() - start_time)\n",
    "                    del x1\n",
    "                    del x2\n",
    "                    del img1\n",
    "                    del img2\n",
    "\n",
    "                    # Perceptual Score\n",
    "                    for num_samples in num_samples_list:\n",
    "\n",
    "                      img1 = img1_tensor[s].unsqueeze(0)\n",
    "                      img2 = img2_tensor[s].unsqueeze(0)  \n",
    "                      shuffle_ids = torch.randperm(len(data))[:num_samples]\n",
    "                      comp_samples1 = img1_tensor[shuffle_ids]\n",
    "                      comp_samples2 = img2_tensor[shuffle_ids]\n",
    "\n",
    "                      x1 = torch.cat([img1, img1.repeat(num_samples,1,1,1), comp_samples1], 0)\n",
    "                      x2 = torch.cat([img2, comp_samples2, img2.repeat(num_samples,1,1,1)], 0)\n",
    "\n",
    "                      start_time = time.time()               \n",
    "                      _ =  model([x1.to(device),x2.to(device)])\n",
    "                      times[\"perc\"][num_samples].append(time.time() - start_time) \n",
    "                      del x1\n",
    "                      del x2\n",
    "                      del img1\n",
    "                      del img2    \n",
    "\n",
    "                  print(\"Save perceptual score\")\n",
    "\n",
    "                  if no_gpu:\n",
    "                    save_path_times = exp_path.replace(\"basic_img\", f\"basic_img_TimeMeasure_single_cpu\")\n",
    "                  else:\n",
    "                    save_path_times = exp_path.replace(\"basic_img\", f\"basic_img_TimeMeasure_single\")\n",
    "\n",
    "                  torch.save(times, save_path_times)\n",
    "\n",
    "                else:\n",
    "                    # Attribution size matches input size: 3x3x32x32\n",
    "                    print(f\"Seed: {seed} facor {f_id+1} of {len(f_list)}\")\n",
    "\n",
    "                    for k in range(int(10000/factor)):\n",
    "                      img1 = img1_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img2 = img2_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img_labels = labels[k*factor:(k+1)*factor].clone()\n",
    "                      lrp = LRP(model)\n",
    "\n",
    "                      start_time = time.time()\n",
    "                      _ = lrp.attribute(torch.stack([img1, img2],0).permute([1,0,2,3,4]).to(device), target=img_labels.squeeze().to(device))\n",
    "                      times[\"lrp_single\"].append(time.time()-start_time)\n",
    "                      del img1\n",
    "                      del img2\n",
    "                      del lrp\n",
    "\n",
    "                    for k in range(int(10000/factor)):\n",
    "                      img1 = img1_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img2 = img2_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img_labels = labels[k*factor:(k+1)*factor].clone()\n",
    "                      lrp = LRP(model)\n",
    "\n",
    "                      start_time = time.time()\n",
    "                      _ = lrp.attribute(torch.repeat_interleave(torch.stack([img1, img2],0), 10, dim=1).permute([1,0,2,3,4]).to(device), target=torch.repeat_interleave(img_labels.view(-1),10, dim=0).to(device))\n",
    "                      times[\"lrp\"].append(time.time()-start_time)\n",
    "                      del img1\n",
    "                      del img2\n",
    "                      del lrp\n",
    "\n",
    "                    model = to_contribution_propagation(model).to(device)\n",
    "                    print(f\"Seed: {seed} facor {f_id+1} of {len(f_list)}\")\n",
    "\n",
    "                    \n",
    "                    # SHAPE Score\n",
    "                    for k in range(int(10000/factor)):\n",
    "                      img1 = img1_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img2 = img2_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img_labels = labels[k*factor:(k+1)*factor].clone()\n",
    "\n",
    "                      zeros1 = torch.zeros_like(img1)\n",
    "                      zeros2 = torch.zeros_like(img2)\n",
    "\n",
    "                      # First Input:    Original combination\n",
    "                      # Second part:    modality 2 random\n",
    "                      #  Third part:    modality 1 random\n",
    "                      x1 = [img1, img1, zeros1]\n",
    "                      x2 = [img2, zeros2, img2]\n",
    "\n",
    "                      x1 = torch.cat(x1, 0)\n",
    "                      x2 = torch.cat(x2, 0)\n",
    "\n",
    "                      start_time = time.time() \n",
    "                      _ =  model([x1.to(device),x2.to(device)])\n",
    "                      times[\"shape\"].append(time.time() - start_time)\n",
    "                      del x1\n",
    "                      del x2\n",
    "                      del img1\n",
    "                      del img2     \n",
    "\n",
    "                    # Single Prediction\n",
    "                    for k in range(int(10000/factor)):\n",
    "                      img1 = img1_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img2 = img2_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img_labels = labels[k*factor:(k+1)*factor].clone()\n",
    "\n",
    "                      start_time = time.time()          \n",
    "                      _ =  model([img1.to(device), img2.to(device)])\n",
    "                      times[\"prediction_single\"].append(time.time() - start_time)\n",
    "                      del img1\n",
    "                      del img2     \n",
    "\n",
    "                    for k in range(int(10000/factor)):\n",
    "                      img1 = img1_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img2 = img2_tensor[k*factor:(k+1)*factor].clone()\n",
    "                      img_labels = labels[k*factor:(k+1)*factor].clone()\n",
    "\n",
    "                      x1 = torch.stack([\n",
    "                                          torch.zeros_like(img1),\n",
    "                                          img1,\n",
    "                                          torch.zeros_like(img1)\n",
    "                                      ], 0)\n",
    "                      \n",
    "                      x2 = torch.stack([\n",
    "                                          torch.zeros_like(img2),\n",
    "                                          torch.zeros_like(img2),\n",
    "                                          img2\n",
    "                                      ], 0)       \n",
    "                          \n",
    "                      # Relevance Forward Propagation\n",
    "                      start_time = time.time() \n",
    "                      _ =  model([x1.to(device),x2.to(device)])\n",
    "                      times[\"RFP\"].append(time.time() - start_time)\n",
    "                      del x1\n",
    "                      del x2\n",
    "                      del img1\n",
    "                      del img2         \n",
    "\n",
    "\n",
    "                    # Perceptual Score\n",
    "                    for num_samples in num_samples_list:\n",
    "\n",
    "                      for k in range(int(10000/factor)):\n",
    "\n",
    "                        img1 = []\n",
    "                        img2 = []\n",
    "                        comp_samples1 = []\n",
    "                        comp_samples2 = []\n",
    "\n",
    "                        for s in range(k*factor,(k+1)*factor):\n",
    "                            shuffle_ids = torch.randperm(len(data))[:num_samples]\n",
    "                            img1.append(img1_tensor[s:s+1].clone())\n",
    "                            img2.append(img2_tensor[s:s+1].clone())\n",
    "                            comp_samples1.append(img1_tensor[shuffle_ids].clone())\n",
    "                            comp_samples2.append(img2_tensor[shuffle_ids].clone())\n",
    "\n",
    "                        img1 = torch.cat(img1, 0)\n",
    "                        img2 = torch.cat(img2, 0)\n",
    "                        comp_samples1 = torch.cat(comp_samples1, 0)\n",
    "                        comp_samples2 = torch.cat(comp_samples2, 0)\n",
    "\n",
    "                        x1 = torch.cat([img1, img1.repeat(num_samples,1,1,1), comp_samples1], 0)\n",
    "                        x2 = torch.cat([img2, comp_samples2, img2.repeat(num_samples,1,1,1)], 0)\n",
    "\n",
    "                        start_time = time.time()               \n",
    "                        _ =  model([x1.to(device),x2.to(device)])\n",
    "                        times[\"perc\"][num_samples].append(time.time() - start_time)\n",
    "                        del x1\n",
    "                        del x2\n",
    "                        del img1\n",
    "                        del img2                \n",
    "                        del comp_samples1\n",
    "                        del comp_samples2\n",
    "\n",
    "                    print(\"Save perceptual score\")\n",
    "\n",
    "                    if no_gpu:\n",
    "                      save_path_times = exp_path.replace(\"basic_img\", f\"basic_img_TimeMeasure_batch{factor}_cpu\")\n",
    "                    else:\n",
    "                      save_path_times = exp_path.replace(\"basic_img\", f\"basic_img_TimeMeasure_batch{factor}\")\n",
    "\n",
    "                    torch.save(times, save_path_times)\n",
    "\n",
    "                del times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
