{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/code\")\n",
    "sys.path.append(\"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from models.NetworkMapper import to_base_propagation, to_contribution_propagation\n",
    "from datasets.SEN12MS import CLASS_NAMES, SEN12MS\n",
    "from ContributionPropagation.Contribution import PerceptualScore;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Statistics\n",
    "def mean_var_perceptual_score(acc, n,N,K):\n",
    "  mu, var = mean_var_multigeometrical(n,N,K)\n",
    "  mu_perc = acc - mu / n\n",
    "  var_perc = var / (n**2)\n",
    "  return mu_perc, var_perc\n",
    "\n",
    "def mean_var_multigeometrical(n,N,K):\n",
    " var = n * (K / N) * ((N - K) / N) * ((N - n) / (N - 1))\n",
    " mu = n * (K / N)\n",
    "\n",
    " return mu, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density over 18106 points!\n",
      "path clear:  /media/jakob/Expansion/DataSets/SEN12MS\n",
      "path cloudy:  /media/jakob/Expansion/DataSets/SEN12MSCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Load]: 100%|██████████| 18106/18106 [00:00<00:00, 140557.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 18106 samples from the sen12ms subset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path clear:  /media/jakob/Expansion/DataSets/SEN12MS\n",
      "path cloudy:  /media/jakob/Expansion/DataSets/SEN12MSCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Load]: 100%|██████████| 18106/18106 [00:00<00:00, 96698.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18106 files with 12666 Cloudy versions!\n",
      "loaded 18106 samples from the sen12ms subset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m sample_name \u001b[38;5;241m=\u001b[39m data1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    113\u001b[0m go_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(chosen_pred[i] \u001b[38;5;129;01min\u001b[39;00m sample_name): go_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m go_on: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m data2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_cloudy = False\n",
    "\n",
    "### Violin plots for Perceptual and RFP\n",
    "plot_bias = False\n",
    "setup = \"All\" #[\"Correct\", \"False\", \"All\"]\n",
    "seed = 42 # 43, 44:\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "\n",
    "root_folder = \"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/0005_SEN12MS_Train_and_Evaluate/Classification/Experiments/\"\n",
    "\n",
    "exp_name = \"pretrained_original\" # ResNet50_OpticalSAR_clear_seed\n",
    "exp_name_cloudy = \"pretrained_original\" # ResNet50_OpticalSAR_cloudy_seed\n",
    "\n",
    "# Load Scores for model trained on clear samples\n",
    "\n",
    "if \"seed\" in exp_name:\n",
    "    res_file_path_clear = os.path.join(root_folder, f\"{exp_name}{seed}\", f\"results.pth\")\n",
    "else:\n",
    "    res_file_path_clear = os.path.join(root_folder, f\"{exp_name}\", f\"results.pth\")\n",
    "\n",
    "#perc_file_path_clear = os.path.join(root_folder, f\"ResNet50_OpticalSAR_clear_seed{seed}\", f\"relevance_perc_score_fullFalse.pth\")\n",
    "#perc_clear = torch.load(perc_file_path_clear)\n",
    "pred_clear = torch.load(res_file_path_clear)\n",
    "\n",
    "if \"seed\" in exp_name:\n",
    "    res_file_path_cloudy = os.path.join(root_folder, f\"{exp_name_cloudy}{seed}\", f\"results.pth\")\n",
    "else:\n",
    "    res_file_path_cloudy = os.path.join(root_folder, f\"{exp_name_cloudy}\", f\"results.pth\")\n",
    "#perc_file_path_cloudy = os.path.join(root_folder, f\"ResNet50_OpticalSAR_cloudy_seed{seed}\", f\"relevance_perc_score_fullFalse.pth\")\n",
    "#perc_cloudy = torch.load(perc_file_path_cloudy)\n",
    "pred_cloudy = torch.load(res_file_path_cloudy)\n",
    "\n",
    "\n",
    "sources = [\"Bias\", \"SAR\", \"Optical\"]\n",
    "\n",
    "chosen = [\"ROIs1868_summer_s2_131_p235\", \n",
    "          \"ROIs1970_fall_s2_141_p635.tif\", \n",
    "          \"ROIs1970_fall_s2_120_p292.tif\",\n",
    "          \"ROIs1868_summer_s2_43_p264.tif\"]\n",
    "\n",
    "\n",
    "num_classes = pred_clear[\"Clear\"][\"Predictionss\"].shape[-1]\n",
    "num_samples = len(pred_cloudy[\"Cloudy\"][\"Predictionss\"][0])\n",
    "\n",
    "if train_cloudy: \n",
    "    chosen_pred = pred_cloudy\n",
    "else:\n",
    "    chosen_pred = pred_clear\n",
    "\n",
    "print(f\"Density over {num_samples} points!\")\n",
    "\n",
    "\n",
    "DATA_DIR_CLEAR = \"/media/jakob/Expansion/DataSets/SEN12MS\"\n",
    "DATA_DIR_CLOUDY = \"/media/jakob/Expansion/DataSets/SEN12MSCR\"\n",
    "LABEL_SPLIT_DIR = \"/media/jakob/Expansion/DataSets/SEN12MS_Splits\"\n",
    "\n",
    "imgTransform = None\n",
    "\n",
    "dataset_clear = SEN12MS(DATA_DIR_CLEAR,\n",
    "                        DATA_DIR_CLOUDY, \n",
    "                        LABEL_SPLIT_DIR, \n",
    "                        img_transform=imgTransform, \n",
    "                        label_type=\"single_label\", \n",
    "                        threshold=0.1, \n",
    "                        subset=\"test\", \n",
    "                        use_s1=True, \n",
    "                        use_s2=True, \n",
    "                        use_rgb=True,\n",
    "                        use_cloudy=False,\n",
    "                        cloud_frac=0,\n",
    "                        igbp_s=True)\n",
    "\n",
    "dataset_cloudy = SEN12MS(DATA_DIR_CLEAR,\n",
    "                        DATA_DIR_CLOUDY, \n",
    "                        LABEL_SPLIT_DIR, \n",
    "                        img_transform=imgTransform, \n",
    "                        label_type=\"single_label\", \n",
    "                        threshold=0.8, \n",
    "                        subset=\"test\", \n",
    "                        use_s1=True, \n",
    "                        use_s2=True, \n",
    "                        use_rgb=True,\n",
    "                        use_cloudy=True,\n",
    "                        cloud_frac=1,\n",
    "                        igbp_s=True)\n",
    "\n",
    "reps = 5\n",
    "\n",
    "labels_str = [\"Bias\", \"Optical\", \"SAR\"]\n",
    "for i in range(10):\n",
    "    sub_ids  = [idx for idx in range(len(dataset_clear)) if dataset_clear.labels_filtered[dataset_clear.samples[idx][\"id\"]] == i]\n",
    "    np.random.shuffle(sub_ids)\n",
    "    sample_ids = sub_ids\n",
    "\n",
    "\n",
    "    if len(sub_ids) == 0: continue\n",
    "    fig = plt.figure(figsize=(30,5))\n",
    "    gs = GridSpec(2, reps*2, figure=fig, height_ratios=[0.5,0.5], width_ratios=reps*[0.5,0.5])\n",
    "\n",
    "    for j in range(5):\n",
    "\n",
    "        sub_sample_id = sample_ids[j]\n",
    "        \n",
    "\n",
    "        # perc_val_1 = perc[sample_ids[j]][\"orig_acc\"] - perc[sample_ids[j]][\"correct_mod2\"] / perc[sample_ids[j]][\"num_samples\"]\n",
    "        # perc_val_2 = perc[sample_ids[j]][\"orig_acc\"] - perc[sample_ids[j]][\"correct_mod1\"] / perc[sample_ids[j]][\"num_samples\"]\n",
    "\n",
    "        data1 = dataset_clear[sub_sample_id]\n",
    "        data2 = dataset_cloudy[sub_sample_id]\n",
    "        sample_name = data1[\"id\"]\n",
    "\n",
    "        go_on = False\n",
    "        if np.any(chosen_pred[i] in sample_name): go_on=True\n",
    "\n",
    "        if not go_on: continue\n",
    "        assert data1[\"id\"] == data2[\"id\"]\n",
    "\n",
    "        ax11 = fig.add_subplot(gs[0:2, 2*j])\n",
    "        ax13 = fig.add_subplot(gs[0, 2*j+1])        \n",
    "        ax23 = fig.add_subplot(gs[1, 2*j+1])       \n",
    "\n",
    "        img_opt_clear = data1[\"image\"][3:0:-1].transpose([1,2,0])\n",
    "        img_opt_cloudy = data2[\"image\"][3:0:-1].transpose([1,2,0])\n",
    "        img_sar = np.concatenate([data1[\"image\"][-2:],0.5*(data1[\"image\"][-1:]+data1[\"image\"][-2:-1])], 0).transpose([1,2,0])\n",
    "        img_sar_check = np.concatenate([data2[\"image\"][-2:],0.5*(data2[\"image\"][-1:]+data2[\"image\"][-2:-1])], 0).transpose([1,2,0])\n",
    "\n",
    "        img = np.concatenate([\n",
    "            np.concatenate([img_opt_clear, np.ones([10, img_sar.shape[0],3]), img_opt_cloudy],0),\n",
    "            np.ones([img_sar.shape[0]*2 + 10, 10, 3]),\n",
    "            np.concatenate([np.ones_like(img_sar), np.ones([10, img_sar.shape[0],3]), img_sar],0)], 1)\n",
    "\n",
    "        assert (img_sar == img_sar_check).all()\n",
    "        # assert (img_opt_clear != img_opt_cloudy).any()\n",
    "        \n",
    "        # axs[i,2*j+1].axis('off')\n",
    "        ax11.axis('off')\n",
    "        ax11.matshow(img)\n",
    "        #ax_tl.set_title(f\"Label\\n{data2['label']}  [{pred[:,sample_ids[i,j]].sum(0).argmax(-1).item()}]\", size=12, y=1)  \n",
    "\n",
    "        pred_pos = chosen_pred[\"Clear\"][\"SampleIDs\"].index(sample_name)\n",
    "\n",
    "        ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][0,pred_pos], label=labels_str[0], color='r')\n",
    "\n",
    "        ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][1,pred_pos].clip(min=0), bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:1,pred_pos].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "        ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][1,pred_pos].clip(max=0), bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:1,pred_pos].clip(max=0).sum(0), color='g')\n",
    "\n",
    "        ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][2,pred_pos].clip(min=0), bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:2,pred_pos].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "        ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][2,pred_pos].clip(max=0), bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:2,pred_pos].clip(max=0).sum(0), color='b')\n",
    "        ax13.set_ylim([-20,20]) \n",
    "        ax13.set_xticks([])\n",
    "    \n",
    "        #ax13.set_title(f\"Class {data1['label']} - Pred {pred_clear['Clear']['Predictionss'][:3,pred_pos].sum(0).argmax(-1).item()}\") #  -  Perc: {perc_val_1:.2f}/{perc_val_2:.2f}\")\n",
    "        \n",
    "        ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][0,pred_pos], label=labels_str[0], color='r')\n",
    "        ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][1,pred_pos].clip(min=0), bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:1,pred_pos].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "        ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][1,pred_pos].clip(max=0), bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:1,pred_pos].clip(max=0).sum(0), color='g')\n",
    "\n",
    "        ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][2,pred_pos].clip(min=0), bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:2,pred_pos].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "        ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][2,pred_pos].clip(max=0), bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:2,pred_pos].clip(max=0).sum(0), color='b')\n",
    "        ax23.set_ylim([-20,20]) \n",
    "        ax23.set_xticks(range(10))\n",
    "        \n",
    "        # ax23.set_title(f\"Class {data1['label']} - Pred {pred_clear['Clear']['Predictionss'][:3,pred_pos].sum(0).argmax(-1).item()}\") #  -  Perc: {perc_val_1:.2f}/{perc_val_2:.2f}\")\n",
    "        \n",
    "\n",
    "        #ax_b.legend(fontsize=10, title_fontsize=15)\n",
    "        #ax23.subplots_adjust(hspace=0.0)  # Adjust the vertical spacing between subplots\n",
    "        ax11.axis('off')\n",
    "        ax11.legend(*ax13.get_legend_handles_labels(), fontsize=10, loc=\"upper right\")\n",
    "        ax11.set_title(f\"Trained Cloudy: {train_cloudy}\" +\\\n",
    "                       f\"\\nClass {CLASS_NAMES[i]}\" + \\\n",
    "                    f\"\\nClear: {CLASS_NAMES[chosen_pred['Clear']['Predictionss'].sum(0)[pred_pos].argmax(-1).item()]}\" + \\\n",
    "                    f\"\\n Cloudy:  {CLASS_NAMES[chosen_pred['Cloudy']['Predictionss'].sum(0)[pred_pos].argmax(-1).item()]}\")\n",
    "        #plt.tight_layout()\n",
    "        #plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=1, wspace=None, hspace=None)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density over 18106 points!\n",
      "path clear:  /media/jakob/Expansion/DataSets/SEN12MS\n",
      "path cloudy:  /media/jakob/Expansion/DataSets/SEN12MSCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Load]: 100%|██████████| 18106/18106 [00:00<00:00, 106750.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 18106 samples from the sen12ms subset test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path clear:  /media/jakob/Expansion/DataSets/SEN12MS\n",
      "path cloudy:  /media/jakob/Expansion/DataSets/SEN12MSCR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Load]: 100%|██████████| 18106/18106 [00:00<00:00, 54054.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18106 files with 12666 Cloudy versions!\n",
      "loaded 18106 samples from the sen12ms subset test\n"
     ]
    }
   ],
   "source": [
    "train_cloudy = False\n",
    "data_avail = False\n",
    "### Violin plots for Perceptual and RFP\n",
    "plot_bias = False\n",
    "setup = \"All\" #[\"Correct\", \"False\", \"All\"]\n",
    "seed = 42 # 43, 44:\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "\n",
    "root_folder = \"/home/jakob/Documents/PhD/Projects/ModalityRelevanceScores/0005_SEN12MS_Train_and_Evaluate/Classification/Experiments/\"\n",
    "\n",
    "exp_name = \"pretrained_original\" # ResNet50_OpticalSAR_clear_seed\n",
    "exp_name_cloudy = \"pretrained_original\" # ResNet50_OpticalSAR_cloudy_seed\n",
    "\n",
    "# Load Scores for model trained on clear samples\n",
    "\n",
    "if \"seed\" in exp_name:\n",
    "    res_file_path_clear = os.path.join(root_folder, f\"{exp_name}{seed}\", f\"results.pth\")\n",
    "else:\n",
    "    res_file_path_clear = os.path.join(root_folder, f\"{exp_name}\", f\"results.pth\")\n",
    "\n",
    "perc_file_path_clear = os.path.join(root_folder, f\"pretrained_original\", f\"relevance_perc_score_fullFalse.pth\")\n",
    "perc_clear = torch.load(perc_file_path_clear)\n",
    "pred_clear = torch.load(res_file_path_clear)\n",
    "\n",
    "if \"seed\" in exp_name:\n",
    "    res_file_path_cloudy = os.path.join(root_folder, f\"{exp_name_cloudy}{seed}\", f\"results.pth\")\n",
    "else:\n",
    "    res_file_path_cloudy = os.path.join(root_folder, f\"{exp_name_cloudy}\", f\"results.pth\")\n",
    "perc_file_path_cloudy = os.path.join(root_folder, f\"pretrained_original\", f\"relevance_perc_score_fullFalse.pth\")\n",
    "perc_cloudy = torch.load(perc_file_path_cloudy)\n",
    "pred_cloudy = torch.load(res_file_path_cloudy)\n",
    "\n",
    "\n",
    "sources = [\"Bias\", \"SAR\", \"Optical\"]\n",
    "\n",
    "num_classes = pred_clear[\"Clear\"][\"Predictionss\"].shape[-1]\n",
    "num_samples = len(pred_cloudy[\"Cloudy\"][\"Predictionss\"][0])\n",
    "\n",
    "if train_cloudy: \n",
    "    chosen_pred = pred_cloudy\n",
    "else:\n",
    "    chosen_pred = pred_clear\n",
    "\n",
    "print(f\"Density over {num_samples} points!\")\n",
    "\n",
    "\n",
    "DATA_DIR_CLEAR = \"/media/jakob/Expansion/DataSets/SEN12MS\"\n",
    "DATA_DIR_CLOUDY = \"/media/jakob/Expansion/DataSets/SEN12MSCR\"\n",
    "LABEL_SPLIT_DIR = \"/media/jakob/Expansion/DataSets/SEN12MS_Splits\"\n",
    "\n",
    "imgTransform = None\n",
    "\n",
    "\n",
    "\n",
    "dataset_clear = SEN12MS(DATA_DIR_CLEAR,\n",
    "                        DATA_DIR_CLOUDY, \n",
    "                        LABEL_SPLIT_DIR, \n",
    "                        img_transform=imgTransform, \n",
    "                        label_type=\"single_label\", \n",
    "                        threshold=0.1, \n",
    "                        subset=\"test\", \n",
    "                        use_s1=True, \n",
    "                        use_s2=True, \n",
    "                        use_rgb=True,\n",
    "                        use_cloudy=False,\n",
    "                        cloud_frac=0,\n",
    "                        igbp_s=True)\n",
    "\n",
    "dataset_cloudy = SEN12MS(DATA_DIR_CLEAR,\n",
    "                        DATA_DIR_CLOUDY, \n",
    "                        LABEL_SPLIT_DIR, \n",
    "                        img_transform=imgTransform, \n",
    "                        label_type=\"single_label\", \n",
    "                        threshold=0.8, \n",
    "                        subset=\"test\", \n",
    "                        use_s1=True, \n",
    "                        use_s2=True, \n",
    "                        use_rgb=True,\n",
    "                        use_cloudy=True,\n",
    "                        cloud_frac=1,\n",
    "                        igbp_s=True)\n",
    "\n",
    "\n",
    "base_folder = \"Example_Images_SEN12MS\"\n",
    "os.makedirs(base_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "max_val = 2000\n",
    "\n",
    "target_folder = os.path.join(\"Example_Images_SEN12MS\", f\"Uploads\")\n",
    "os.makedirs(target_folder, exist_ok=True)    \n",
    "labels_str = [\"Bias\", \"SAR\", \"Optical\"]\n",
    "for sub_sample_id in range(len(dataset_cloudy)):\n",
    "\n",
    "    s = dataset_clear.samples[sub_sample_id][\"id\"]\n",
    "    if s not in [\"ROIs1868_summer_s2_131_p235.tif\",\n",
    "                    \"ROIs1868_summer_s2_131_p235.tif\",\n",
    "                    \"ROIs1970_fall_s2_141_p635.tif\",\n",
    "                    \"ROIs1970_fall_s2_120_p292.tif\", \n",
    "                    \"ROIs1868_summer_s2_43_p264.tif\"]: continue\n",
    "\n",
    "    perc_val_1_clear = perc_clear[\"Clear\"][s][\"Accuracy\"] - perc_clear[\"Clear\"][s][\"Correct SAR\"] / perc_clear[\"Clear\"][s][\"Total Count\"]\n",
    "    perc_val_2_clear = perc_clear[\"Clear\"][s][\"Accuracy\"] - perc_clear[\"Clear\"][s][\"Correct Opt\"] / perc_clear[\"Clear\"][s][\"Total Count\"]\n",
    "    perc_val_1_cloudy = perc_val_1_clear\n",
    "    perc_val_2_cloudy = perc_val_2_clear\n",
    "    \n",
    "\n",
    "    data1 = dataset_clear[sub_sample_id]\n",
    "    data2 = dataset_cloudy[sub_sample_id]\n",
    "    sample_name = data1[\"id\"]\n",
    "\n",
    "    assert data1[\"id\"] == data2[\"id\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    gs = GridSpec(2, 2, figure=fig, height_ratios=[0.5,0.5], width_ratios=[0.5,0.5])\n",
    "    sub_sample_id = sample_ids[j]\n",
    "\n",
    "    ax11 = fig.add_subplot(gs[0:2, 0])\n",
    "    ax13 = fig.add_subplot(gs[0, 1])        \n",
    "    ax23 = fig.add_subplot(gs[1, 1])       \n",
    "\n",
    "    img_opt_clear = np.clip(data1[\"image\"][3:0:-1].transpose([1,2,0]), 0, max_val) / max_val\n",
    "    img_opt_cloudy = np.clip(data2[\"image\"][3:0:-1].transpose([1,2,0]), 0, max_val) / max_val\n",
    "    img_sar = np.concatenate([data1[\"image\"][-2:],0.5*(data1[\"image\"][-1:]+data1[\"image\"][-2:-1])], 0).transpose([1,2,0])\n",
    "    img_sar_check = np.concatenate([data2[\"image\"][-2:],0.5*(data2[\"image\"][-1:]+data2[\"image\"][-2:-1])], 0).transpose([1,2,0])\n",
    "\n",
    "    img = np.concatenate([\n",
    "        np.concatenate([img_opt_clear, np.ones([10, img_sar.shape[0],3]), img_opt_cloudy],0),\n",
    "        np.ones([img_sar.shape[0]*2 + 10, 10, 3]),\n",
    "        np.concatenate([np.ones_like(img_sar), np.ones([10, img_sar.shape[0],3]), img_sar],0)], 1)\n",
    "\n",
    "    assert (img_sar == img_sar_check).all()\n",
    "    # assert (img_opt_clear != img_opt_cloudy).any()\n",
    "    \n",
    "    # axs[i,2*j+1].axis('off')\n",
    "    ax11.axis('off')\n",
    "    ax11.matshow(img)\n",
    "    #ax_tl.set_title(f\"Label\\n{data2['label']}  [{pred[:,sample_ids[i,j]].sum(0).argmax(-1).item()}]\", size=12, y=1)  \n",
    "\n",
    "    pred_pos = chosen_pred[\"Clear\"][\"SampleIDs\"].index(sample_name)\n",
    "\n",
    "    text_size = 18\n",
    "    ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][0,pred_pos], label=labels_str[0], color='r')\n",
    "    ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][1,pred_pos].clip(min=0), width=0.8, bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:1,pred_pos].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "    ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][1,pred_pos].clip(max=0), width=0.8, bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:1,pred_pos].clip(max=0).sum(0), color='g')\n",
    "    ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][2,pred_pos].clip(min=0), width=0.8, bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:2,pred_pos].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "    ax13.bar(range(10), chosen_pred[\"Clear\"][\"Predictionss\"][2,pred_pos].clip(max=0), width=0.8, bottom=chosen_pred[\"Clear\"][\"Predictionss\"][:2,pred_pos].clip(max=0).sum(0), color='b')\n",
    "    ax13.bar(np.arange(10)+0.2, chosen_pred[\"Clear\"][\"Predictionss\"][:3,pred_pos].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax13.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "\n",
    "    ax13.set_ylim([-40,40]) \n",
    "    ax13.tick_params(axis='x', labelsize=text_size)\n",
    "    ax13.tick_params(axis='y', labelsize=text_size)\n",
    "    ax13.set_xticks(range(10), CLASS_NAMES, rotation=45, ha=\"right\", fontsize=text_size)\n",
    "\n",
    "    # ax13.title(f\"Class {CLASS_NAMES[label]}  -  Prediction {pred[:3,sub_sample_id].sum(0).argmax(-1).item()}  -  Perc. Scores: {perc_val_1:.2f} / {perc_val_2:.2f}\", fontsize=text_size)\n",
    "    #ax13.legend(fontsize=text_size, title_fontsize=text_size)\n",
    "\n",
    "\n",
    "\n",
    "    text_size = 18\n",
    "    ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][0,pred_pos], label=labels_str[0], color='r')\n",
    "    ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][1,pred_pos].clip(min=0), width=0.8, bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:1,pred_pos].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "    ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][1,pred_pos].clip(max=0), width=0.8, bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:1,pred_pos].clip(max=0).sum(0), color='g')\n",
    "    ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][2,pred_pos].clip(min=0), width=0.8, bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:2,pred_pos].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "    ax23.bar(range(10), chosen_pred[\"Cloudy\"][\"Predictionss\"][2,pred_pos].clip(max=0), width=0.8, bottom=chosen_pred[\"Cloudy\"][\"Predictionss\"][:2,pred_pos].clip(max=0).sum(0), color='b')\n",
    "    ax23.bar(np.arange(10)+0.2, chosen_pred[\"Cloudy\"][\"Predictionss\"][:3,pred_pos].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax23.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "    ax23.tick_params(axis='x', labelsize=text_size)\n",
    "    ax23.tick_params(axis='y', labelsize=text_size)\n",
    "    ax23.set_ylim([-40,40]) \n",
    "    ax23.set_xticks(range(10), CLASS_NAMES, rotation=45, ha=\"right\", fontsize=text_size)\n",
    "\n",
    "    # ax23.title(f\"Class {CLASS_NAMES[label]}  -  Prediction {pred[:3,sub_sample_id].sum(0).argmax(-1).item()}  -  Perc. Scores: {perc_val_1:.2f} / {perc_val_2:.2f}\", fontsize=text_size)\n",
    "    #ax23.legend(fontsize=text_size, title_fontsize=text_size)\n",
    "    \n",
    "\n",
    "\n",
    "    ax11.axis('off')\n",
    "    ax11.legend(*ax13.get_legend_handles_labels(), fontsize=text_size, loc=\"upper right\")\n",
    "    ax11.set_title(f\"\\nTrue Class: {CLASS_NAMES[i]}\" + \\\n",
    "                f\"\\nClear Prediction: {CLASS_NAMES[chosen_pred['Clear']['Predictionss'].sum(0)[pred_pos].argmax(-1).item()]}\" + \\\n",
    "                f\"\\nCloudy Prediction:  {CLASS_NAMES[chosen_pred['Cloudy']['Predictionss'].sum(0)[pred_pos].argmax(-1).item()]}\",\n",
    "                fontsize=text_size)\n",
    "    \n",
    "\n",
    "    #ax11.set_title(f\"Perc. Scores: {perc_val_1_clear} / {perc_val_2_clear}\\n\" +\\\n",
    "    #                f\"Perc. Scores: {perc_val_1_cloudy} / {perc_val_1_cloudy}\",\n",
    "    #            fontsize=text_size)\n",
    "    \n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    #plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=1, wspace=None, hspace=None)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(target_folder, f\"{data1['id']}.png\"), dpi=180)\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
