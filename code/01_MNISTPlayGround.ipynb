{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Forward Propagation (RFP) for Multi-Source MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.multisource_mnist import MSMNIST;\n",
    "from models.networks.small_net import SmallNet;\n",
    "from models.network_mapper import to_relevance_representation, to_basic_representation;\n",
    "from utils.Utils import input_mapping, set_seed\n",
    "import matplotlib.pyplot as plt;\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from torch.utils.data import DataLoader;\n",
    "import seaborn as sns\n",
    "\n",
    "import torch;\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Setup\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| label_summation | True or False | Shuffle images and sum up labels modulo ten |\n",
    "| split_image | True or False | Split image vertically or give full image to both data sources |\n",
    "| frac_data_noise1 | Any float value between zero and one | Fraction of noisy examples in Data Source 1 |\n",
    "| frac_data_noies2 | Any float value between zero and one | Fraction of noisy examples in Data Source 2 |\n",
    "| frac_label_noise1 | Any float value between zero and one | Fraction of random labels in Data Source 1 |\n",
    "| frac_label_noies2 | Any float value between zero and one | Fraction of random labels in Data Source 2 |\n",
    "| shuffle1 |  True or False | Shuffle pixels of Data Source 1 |\n",
    "| shuffle2 | True or False | Shuffle pixels of Data Source 2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "label_summation = False\n",
    "split_image = True\n",
    "frac_data_noise1 = 0\n",
    "frac_data_noise2 = 0\n",
    "frac_label_noise1 = 0\n",
    "frac_label_noise2 = 0\n",
    "\n",
    "shuffle1 = False\n",
    "shuffle2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "train_data = MSMNIST(train=True, \n",
    "                     label_summation=label_summation, \n",
    "                     split_image=split_image, \n",
    "                     frac_data_noise1=frac_data_noise1,\n",
    "                     frac_data_noise2=frac_data_noise2,\n",
    "                     frac_label_noise1=frac_label_noise1,\n",
    "                     frac_label_noise2=frac_label_noise2,\n",
    "                     shuffle1=shuffle1,\n",
    "                     shuffle2=shuffle2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Visualization\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| num_examples | [num_rows, num_cols] | Number of rows and columns of example images |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "num_examples = [5,8]\n",
    "figsize = [15,7]\n",
    "fontsize = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(43)\n",
    "fig, axs = plt.subplots(*num_examples, figsize=figsize)\n",
    "train_data.plot_imgs(axs, num_examples, fontsize=fontsize)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| num_epochs | Any int greater zero | Number of epochs to train |\n",
    "| batch_size | Any int greater zero | Batch size for training and evaluation |\n",
    "| model_save_path | any path to (non-) existing file (Default: None)| path to save model to and load from. None for no saving or loading|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "model_save_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = SmallNet(num_classes=10, input_shape=[28,14] if split_image else [28,28]).to(device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  correct_count = 0\n",
    "  total_count = 0\n",
    "  loss_sum = 0\n",
    "  for i, data in enumerate(train_data_loader):\n",
    "    print(f\"Epoch [{epoch+1} / {num_epochs}]  -  Iter [{i+1} / {len(train_data_loader)}]    \", end=\"\\r\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    label = data[\"label\"].to(device)\n",
    "    x1 = data[\"img1\"].to(device)\n",
    "    x2 = data[\"img2\"].to(device)\n",
    "    pred = model(x1,x2)\n",
    "\n",
    "    loss = criterion(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_sum += loss.detach().cpu().item()\n",
    "    correct_count += (label == pred.argmax(-1)).sum().cpu().item()\n",
    "    total_count += len(x1)\n",
    "  \n",
    "  print(f\"Epoch [{epoch+1} / {num_epochs}]  -  Iter [{i+1} / {len(train_data_loader)}]  -  Loss {loss_sum/len(train_data_loader):.4f}  -  Acc {correct_count/total_count:.4f}\")\n",
    "\n",
    "if model_save_path is not None:\n",
    "  print(\"Save trained model: \", model_save_path)\n",
    "  torch.save({\"final_model_state_dict\": model.state_dict()}, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "#### Load Test Data\n",
    "**Data Setup**\n",
    "| Variable | Options | Explanation |\n",
    "| --- | --- | --- |\n",
    "| frac_data_noise1 | Any float value between zero and one | Fraction of noisy examples in Data Source 1 (usually 0 or 1 for testing)|\n",
    "| frac_data_noies2 | Any float value between zero and one | Fraction of noisy examples in Data Source 2 (usually 0 or 1 for testing) |\n",
    "| frac_label_noise1 | Any float value between zero and one | Fraction of random labels in Data Source 1 (usually 0 or 1 for testing) |\n",
    "| frac_label_noies2 | Any float value between zero and one | Fraction of random labels in Data Source 2 (usually 0 or 1 for testing) |\n",
    "| shuffle1 |  True or False | Shuffle pixels of Data Source 1 (usually equivalent to training for testing)|\n",
    "| shuffle2 | True or False | Shuffle pixels of Data Source 2 (usually equivalent to training for testing)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Setup\n",
    "frac_data_noise1_test = 0\n",
    "frac_data_noise2_test = 0\n",
    "frac_label_noise1_test = 0\n",
    "frac_label_noise2_test = 0\n",
    "\n",
    "shuffle1_test = shuffle1\n",
    "shuffle2_test = shuffle2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(43)\n",
    "test_data = MSMNIST(train=False, \n",
    "                     label_summation=label_summation, \n",
    "                     split_image=split_image, \n",
    "                     frac_data_noise1=frac_data_noise1_test,\n",
    "                     frac_data_noise2=frac_data_noise2_test,\n",
    "                     frac_label_noise1=frac_label_noise1_test,\n",
    "                     frac_label_noise2=frac_label_noise2_test,\n",
    "                     shuffle1=shuffle1_test,\n",
    "                     shuffle2=shuffle2_test);\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "fig, axs = plt.subplots(*num_examples, figsize=figsize)\n",
    "test_data.plot_imgs(axs, num_examples, fontsize=fontsize)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate Model\")\n",
    "\n",
    "model.eval()\n",
    "model = model.double().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_correct_count, test_correct_count = 0, 0\n",
    "  train_total_count, test_total_count = 0, 0\n",
    "  train_loss_sum, test_loss_sum = 0, 0\n",
    "  train_pred_list = []\n",
    "  test_pred_list = []\n",
    "\n",
    "  for i, data in enumerate(train_data_loader):\n",
    "    print(f\"Eval Train Performance [{i+1} / {len(train_data_loader)}]    \", end=\"\\r\")\n",
    "    label = data[\"label\"].to(device)\n",
    "    x1 = data[\"img1\"].double().to(device)\n",
    "    x2 = data[\"img2\"].double().to(device)\n",
    "    pred = model(x1,x2)\n",
    "\n",
    "    train_pred_list.append(pred.cpu())\n",
    "    train_loss = criterion(pred, label)\n",
    "\n",
    "    train_loss_sum += train_loss.cpu().item()\n",
    "    train_correct_count += (label == pred.argmax(-1)).sum().cpu().item()\n",
    "    train_total_count += len(x1)\n",
    "  print(f\"Eval Train Performance [{i+1} / {len(train_data_loader)}]    \" +\\\n",
    "        f\"Loss {train_loss_sum/len(train_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {train_correct_count/train_total_count:.4f}\")\n",
    "\n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    \", end=\"\\r\")\n",
    "    label = data[\"label\"].to(device)\n",
    "    x1 = data[\"img1\"].double().to(device)\n",
    "    x2 = data[\"img2\"].double().to(device)\n",
    "    pred = model(x1,x2)\n",
    "\n",
    "    test_pred_list.append(pred.cpu())\n",
    "    test_loss = criterion(pred, label)\n",
    "\n",
    "    test_loss_sum += test_loss.cpu().item()\n",
    "    test_correct_count += (label == pred.argmax(-1)).sum().cpu().item()\n",
    "    test_total_count += x1.shape[-4]\n",
    "\n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {test_loss_sum/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {test_correct_count/test_total_count:.4f}\")\n",
    "  \n",
    "  train_pred_list = torch.cat(train_pred_list, 0)\n",
    "  test_pred_list = torch.cat(test_pred_list, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate RFP-Model\")\n",
    "\n",
    "model = to_relevance_representation(model=model, verbose=0)\n",
    "model = model.double().to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  rfp_train_pred_list = []\n",
    "  rfp_test_pred_list = []\n",
    "  rfp_labels = []\n",
    "\n",
    "  rfp_train_correct_count, rfp_test_correct_count = 0, 0\n",
    "  rfp_train_total_count, rfp_test_total_count = 0, 0\n",
    "  rfp_train_loss_sum, rfp_test_loss_sum = 0, 0\n",
    "\n",
    "  for i, data in enumerate(train_data_loader):\n",
    "    print(f\"Eval Train Performance [{i+1} / {len(train_data_loader)}]    \", end=\"\\r\")\n",
    "    label = data[\"label\"].to(device)\n",
    "\n",
    "    x1, x2 = input_mapping(data[\"img1\"], data[\"img2\"])\n",
    "\n",
    "    pred = model(x1.double().to(device),x2.double().to(device))\n",
    "\n",
    "    rfp_train_pred_list.append(pred.cpu())\n",
    "\n",
    "    rfp_train_loss_sum += criterion(pred.sum(0), label).cpu().item()\n",
    "    rfp_train_correct_count += (label == pred.sum(0).argmax(-1)).sum().cpu().item()\n",
    "    rfp_train_total_count += x1.shape[-4]\n",
    "    \n",
    "  print(f\"Eval Train Performance [{i+1} / {len(train_data_loader)}]     \" +\\\n",
    "        f\"Loss {rfp_train_loss_sum/len(train_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {rfp_train_correct_count/rfp_train_total_count:.4f}\")\n",
    "\n",
    "  for i, data in enumerate(test_data_loader):\n",
    "    print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]    \", end=\"\\r\")\n",
    "    label = data[\"label\"].to(device)\n",
    "\n",
    "    x1, x2 = input_mapping(data[\"img1\"], data[\"img2\"])\n",
    "\n",
    "    pred = model(x1.double().to(device),x2.double().to(device))\n",
    "\n",
    "    rfp_test_pred_list.append(pred.cpu())\n",
    "    rfp_labels.append(label.cpu())\n",
    "\n",
    "    rfp_test_loss_sum += criterion(pred.sum(0), label).cpu().item()\n",
    "    rfp_test_correct_count += (label == pred.sum(0).argmax(-1)).sum().cpu().item()\n",
    "    rfp_test_total_count += x1.shape[-4]\n",
    "    \n",
    "  print(f\"Eval Test Performance [{i+1} / {len(test_data_loader)}]     \" +\\\n",
    "        f\"Loss {rfp_test_loss_sum/len(test_data_loader):.4f}    \"+\\\n",
    "        f\"Acc {rfp_test_correct_count/rfp_test_total_count:.4f}\")\n",
    "  \n",
    "model = to_basic_representation(model=model, verbose=0)\n",
    "\n",
    "rfp_labels = torch.cat(rfp_labels, 0)\n",
    "rfp_train_pred_list = torch.cat(rfp_train_pred_list, -2)\n",
    "rfp_test_pred_list = torch.cat(rfp_test_pred_list, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity Evaluation RFP and Basic Model\")\n",
    "print()\n",
    "print(\"Output shapes\")\n",
    "print(f\"    Basic Model:   Train {train_pred_list.shape}   Test {test_pred_list.shape}\")\n",
    "print(f\"      RFP Model:   Train {rfp_train_pred_list.shape}   Test {rfp_test_pred_list.shape}\")\n",
    "print(f\"Loss:\")\n",
    "print(f\"    Basic Model:   Train {train_loss_sum/len(train_data_loader):.6f}     Test {test_loss_sum/len(test_data_loader):.6f}\")\n",
    "print(f\"      RFP Model:   Train {rfp_train_loss_sum/len(train_data_loader):.6f}     Test {rfp_test_loss_sum/len(test_data_loader):.6f}\")\n",
    "print(f\"          Delta:   Train {(train_loss_sum-rfp_train_loss_sum)/len(train_data_loader):.6f}     Test {(test_loss_sum-rfp_test_loss_sum)/len(test_data_loader):.6f}\")\n",
    "print()\n",
    "print(\"Model Output\")\n",
    "print(f\"    Mean L1-error:   Train {(train_pred_list-rfp_train_pred_list.sum(0)).abs().mean()}     Test {(test_pred_list-rfp_test_pred_list.sum(0)).abs().mean()}\")\n",
    "print(f\"     Std L1-error:   Train {(train_pred_list-rfp_train_pred_list.sum(0)).abs().std()}     Test {(test_pred_list-rfp_test_pred_list.sum(0)).abs().std()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Visualizations\n",
    "Visualization of sample-wise relevance of data source 1 (L), data source 2 (R) and the sample-wise difference (L-R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses contributions and labels of previous cells\n",
    "\n",
    "data = {}\n",
    "\n",
    "num_classes = rfp_test_pred_list.shape[-1]\n",
    "num_samples = len(rfp_test_pred_list[0])\n",
    "\n",
    "print(f\"Density over {num_samples} points!\")\n",
    "text_size = 16\n",
    "title_size = 18\n",
    "fig, axs = plt.subplots(nrows=1, ncols=num_classes+1, figsize=(20,5))\n",
    "\n",
    "data[\"Contribution\"]  =  []\n",
    "data[\"Source\"] = []\n",
    "\n",
    "for i in range(num_classes+1):\n",
    "\n",
    "  if i > 0:\n",
    "    ids_filter = [k for k in range(len(rfp_labels)) if rfp_labels[k] == i-1]\n",
    "    data[\"Contribution\"] = [rfp_test_pred_list[1,id,i-1].item() for id in ids_filter] + \\\n",
    "                            [rfp_test_pred_list[2,id,i-1].item() for id in ids_filter] + \\\n",
    "                            [rfp_test_pred_list[1,id,i-1].item()-rfp_test_pred_list[2,id,i-1].item() for id in ids_filter]\n",
    "  else:\n",
    "    ids_filter = range(len(rfp_labels))\n",
    "    data[\"Contribution\"] = [rfp_test_pred_list[1,id,rfp_labels[id]].item() for id in ids_filter] + \\\n",
    "                            [rfp_test_pred_list[2,id,rfp_labels[id]].item() for id in ids_filter] + \\\n",
    "                            [rfp_test_pred_list[1,id,rfp_labels[id]].item()-rfp_test_pred_list[2,id,rfp_labels[id]].item() for id in ids_filter]\n",
    "\n",
    "\n",
    "  data[\"Source\"] = [\"L\"] * len(ids_filter) + [\"R\"] * len(ids_filter) + [\"L-R\"] * len(ids_filter)\n",
    "\n",
    "  sns.violinplot(data=data,\n",
    "                  x=\"Source\",\n",
    "                  y=\"Contribution\",\n",
    "                  hue=\"Source\",\n",
    "                  split=False, \n",
    "                  ax=axs[i],\n",
    "                  orient=\"v\",\n",
    "                  saturation=0.8\n",
    "                  )\n",
    "  \n",
    "  if i==0:\n",
    "    axs[i].set_ylabel('Relevance Value (RFP)', fontsize=text_size)\n",
    "    axs[i].tick_params(axis='x', labelsize=text_size)\n",
    "    axs[i].tick_params(axis='y', labelsize=text_size)\n",
    "\n",
    "    axs[i].tick_params(axis='x', labelsize=text_size)\n",
    "    axs[i].tick_params(axis='y', labelsize=text_size)\n",
    "  else:\n",
    "    axs[i].tick_params(axis='x', labelsize=text_size)\n",
    "    axs[i].tick_params(axis='y', labelsize=text_size)\n",
    "    axs[i].get_yaxis().set_visible(False)\n",
    "\n",
    "  axs[i].set_ylim([-35,35])\n",
    "  axs[i].hlines(0, -0.5, 2.5, linestyle=\"--\", color=\"red\")\n",
    "\n",
    "  if i == 0:\n",
    "    axs[i].set_title(f\"Total\", fontsize=text_size)  \n",
    "  else:\n",
    "    axs[i].set_title(f\"Class {i-1}\", fontsize=text_size)\n",
    "  \n",
    "  axs[i].set_xlabel('')\n",
    "\n",
    "fig.suptitle(f\"Class-wise Data Source Relevance on Data Fusion MNIST\", fontsize=title_size)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Visualization\n",
    "\n",
    "| Plot Number | Number of Columns | Number of Rows | Content |\n",
    "| --- | ---| --- | --- |\n",
    "| 1st Plot | ncols | 10 | ncols examples per class \n",
    "| 2nd Plot | ncols | nrows | examples of correct predictions\n",
    "| 3rd Plot | ncols | nrows | examples of false predictions\n",
    "\n",
    "The parameters have to be set individually in each of the next three cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class-wise Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 2\n",
    "\n",
    "set_seed(42)\n",
    "fig = plt.figure(figsize=(30,60))\n",
    "fontsize = 24\n",
    "\n",
    "gs = GridSpec(2*nrows, 2*ncols, \n",
    "              figure=fig, \n",
    "              height_ratios=nrows * [0.5,0.5], \n",
    "              width_ratios=ncols * [0.5, 1])\n",
    "\n",
    "labels_str = [\"Bias\", \"Left Part\", \"Right Part\"]\n",
    "for i in range(10):\n",
    "\n",
    "  sample_ids = [idx for idx in torch.randperm(len(test_data)) if test_data.orig_labels1[idx] == i]\n",
    "  \n",
    "  for j in range(ncols):\n",
    "\n",
    "    sub_sample_id = sample_ids[j]\n",
    "\n",
    "    data1 = test_data[sub_sample_id]\n",
    "    data2 = test_data[sub_sample_id]\n",
    "\n",
    "    ax_tl = fig.add_subplot(gs[2*i, 2*j])\n",
    "    ax_tr = fig.add_subplot(gs[2*i+1, 2*j])\n",
    "    ax_b = fig.add_subplot(gs[2*i:2*i+2, 2*j+1])\n",
    "    \n",
    "    ax_tl.axis('off')\n",
    "    ax_tl.matshow(torch.cat([data1[\"img1\"][0], torch.ones([28,5]), data2[\"img2\"][0]], 1), cmap=\"gray\")\n",
    "\n",
    "    plt.bar(range(10), rfp_test_pred_list[0,sample_ids[j]], label=labels_str[0], color='r')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(max=0).sum(0), color='g')\n",
    "\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(max=0).sum(0), color='b')\n",
    "    \n",
    "    ax_b.bar(torch.arange(10)+0.2, rfp_test_pred_list[:3,sample_ids[j]].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax_b.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "    ax_b.set_ylim([-20,40])     \n",
    "    ax_b.set_xticks(range(10))\n",
    "\n",
    "    ax_b.set_xticks(range(10), range(10), fontsize=fontsize)\n",
    "    ax_b.set_yticks(range(-20,41,10), range(-20,41,10), fontsize=fontsize)\n",
    "\n",
    "    ax_b.set_title(f\"Class {data1['label']} - Pred {rfp_test_pred_list[:3,sub_sample_id].sum(0).argmax(-1).item()}\", fontsize=fontsize)\n",
    "    ax_tr.axis('off')\n",
    "    ax_tr.legend(*ax_b.get_legend_handles_labels(), fontsize=fontsize)\n",
    "\n",
    "  fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "ncols = 2\n",
    "\n",
    "set_seed(45)\n",
    "fig = plt.figure(figsize=(30,12))\n",
    "fontsize = 24\n",
    "\n",
    "gs = GridSpec(2*nrows, 2*ncols, \n",
    "              figure=fig, \n",
    "              height_ratios=nrows * [0.5,0.5], \n",
    "              width_ratios=ncols * [0.5, 1])\n",
    "\n",
    "labels_str = [\"Bias\", \"Left Part\", \"Right Part\"]\n",
    "preds = rfp_test_pred_list.sum(0).argmax(-1)\n",
    "for i in range(nrows):\n",
    "\n",
    "  sample_ids = [idx for idx in torch.randperm(len(test_data)) if test_data.orig_labels1[idx] == preds[idx]]\n",
    "  \n",
    "  for j in range(ncols):\n",
    "\n",
    "    sub_sample_id = sample_ids[j]\n",
    "\n",
    "    data1 = test_data[sub_sample_id]\n",
    "    data2 = test_data[sub_sample_id]\n",
    "\n",
    "    ax_tl = fig.add_subplot(gs[2*i, 2*j])\n",
    "    ax_tr = fig.add_subplot(gs[2*i+1, 2*j])\n",
    "    ax_b = fig.add_subplot(gs[2*i:2*i+2, 2*j+1])\n",
    "    \n",
    "    ax_tl.axis('off')\n",
    "    ax_tl.matshow(torch.cat([data1[\"img1\"][0], torch.ones([28,5]), data2[\"img2\"][0]], 1), cmap=\"gray\")\n",
    "\n",
    "    plt.bar(range(10), rfp_test_pred_list[0,sample_ids[j]], label=labels_str[0], color='r')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(max=0).sum(0), color='g')\n",
    "\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(max=0).sum(0), color='b')\n",
    "    \n",
    "    ax_b.bar(torch.arange(10)+0.2, rfp_test_pred_list[:3,sample_ids[j]].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax_b.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "    ax_b.set_ylim([-20,40])     \n",
    "    ax_b.set_xticks(range(10))\n",
    "\n",
    "    ax_b.set_xticks(range(10), range(10), fontsize=fontsize)\n",
    "    ax_b.set_yticks(range(-20,41,10), range(-20,41,10), fontsize=fontsize)\n",
    "\n",
    "    if label_summation:\n",
    "      ax_b.set_title(f\"Class {(data1['label']+data2['label'])%10} - Pred {rfp_test_pred_list[:3,sub_sample_id].sum(0).argmax(-1).item()}\", fontsize=fontsize)\n",
    "    else:\n",
    "      ax_b.set_title(f\"Class {data1['label']} - Pred {rfp_test_pred_list[:3,sub_sample_id].sum(0).argmax(-1).item()}\", fontsize=fontsize)\n",
    "    ax_tr.axis('off')\n",
    "    ax_tr.legend(*ax_b.get_legend_handles_labels(), fontsize=fontsize)\n",
    "\n",
    "  fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random False Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 10\n",
    "ncols = 2\n",
    "\n",
    "set_seed(42)\n",
    "fig = plt.figure(figsize=(30,60))\n",
    "fontsize = 24\n",
    "\n",
    "gs = GridSpec(2*nrows, 2*ncols, \n",
    "              figure=fig, \n",
    "              height_ratios=nrows * [0.5,0.5], \n",
    "              width_ratios=ncols * [0.5, 1])\n",
    "\n",
    "labels_str = [\"Bias\", \"Left Part\", \"Right Part\"]\n",
    "preds = rfp_test_pred_list.sum(0).argmax(-1)\n",
    "for i in range(nrows):\n",
    "\n",
    "  sample_ids = [idx for idx in torch.randperm(len(test_data)) if test_data.orig_labels1[idx] != preds[idx]]\n",
    "  \n",
    "  for j in range(ncols):\n",
    "\n",
    "    sub_sample_id = sample_ids[j]\n",
    "\n",
    "    data1 = test_data[sub_sample_id]\n",
    "    data2 = test_data[sub_sample_id]\n",
    "\n",
    "    ax_tl = fig.add_subplot(gs[2*i, 2*j])\n",
    "    ax_tr = fig.add_subplot(gs[2*i+1, 2*j])\n",
    "    ax_b = fig.add_subplot(gs[2*i:2*i+2, 2*j+1])\n",
    "    \n",
    "    ax_tl.axis('off')\n",
    "    ax_tl.matshow(torch.cat([data1[\"img1\"][0], torch.ones([28,5]), data2[\"img2\"][0]], 1), cmap=\"gray\")\n",
    "\n",
    "    plt.bar(range(10), rfp_test_pred_list[0,sample_ids[j]], label=labels_str[0], color='r')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(min=0).sum(0), label=labels_str[1], color='g')\n",
    "    plt.bar(range(10), rfp_test_pred_list[1,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:1,sample_ids[j]].clip(max=0).sum(0), color='g')\n",
    "\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(min=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(min=0).sum(0), label=labels_str[2], color='b')\n",
    "    ax_b.bar(range(10), rfp_test_pred_list[2,sample_ids[j]].clip(max=0), bottom=rfp_test_pred_list[:2,sample_ids[j]].clip(max=0).sum(0), color='b')\n",
    "    \n",
    "    ax_b.bar(torch.arange(10)+0.2, rfp_test_pred_list[:3,sample_ids[j]].sum(0), label=\"Output\", width=0.4, color='gray', alpha=0.85, linestyle=\"--\", edgecolor=\"black\")\n",
    "    ax_b.hlines(y=0,xmin=-0.5, xmax=9.5, color=\"black\")\n",
    "    ax_b.set_ylim([-20,40])     \n",
    "    ax_b.set_xticks(range(10))\n",
    "\n",
    "    ax_b.set_xticks(range(10), range(10), fontsize=fontsize)\n",
    "    ax_b.set_yticks(range(-20,41,10), range(-20,41,10), fontsize=fontsize)\n",
    "\n",
    "    ax_b.set_title(f\"Class {data1['label']} - Pred {rfp_test_pred_list[:3,sub_sample_id].sum(0).argmax(-1).item()}\", fontsize=fontsize)\n",
    "    ax_tr.axis('off')\n",
    "    ax_tr.legend(*ax_b.get_legend_handles_labels(), fontsize=fontsize)\n",
    "\n",
    "  fig.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
